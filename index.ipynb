{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q pypdf litellm openai scikit-learn numpy upstash_vector cohere tqdm sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import uuid\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "reader = PdfReader(\"judgment.pdf\")\n",
    "text = ''\n",
    "for page_number in range(len(reader.pages)):\n",
    "    text += reader.pages[page_number].extract_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class for Chunking, Embedding and Processing Extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunker:\n",
    "    def __init__(self, buffer_size: int = 1, embedding_client=None,breakpoint_percentile_threshold=90):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.client = embedding_client\n",
    "        self.breakpoint_percentile_threshold = breakpoint_percentile_threshold\n",
    "        self.abbreviations = {'E.g.', 'e.g.', 'i.e.', 'etc.'}\n",
    "\n",
    "    def combine_sentences(self, sentences: List[dict]) -> List[dict]:\n",
    "        for i in range(len(sentences)):\n",
    "            combined_sentence = \"\"\n",
    "            for j in range(max(0, i - self.buffer_size), i + 1 + self.buffer_size):\n",
    "                if 0 <= j < len(sentences):\n",
    "                    combined_sentence += (sentences[j][\"sentence\"] + \" \").strip()\n",
    "            sentences[i][\"combined_sentence\"] = combined_sentence.strip()\n",
    "        return sentences\n",
    "\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        response = self.client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        return np.array(response.data[0].embedding)\n",
    "\n",
    "    def calculate_cosine_distances(self, sentences: List[dict]) -> Tuple[List[float], List[dict]]:\n",
    "        distances = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            embedding_current = sentences[i][\"combined_sentence_embedding\"]\n",
    "            embedding_next = sentences[i + 1][\"combined_sentence_embedding\"]\n",
    "            similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "            distance = 1 - similarity\n",
    "            distances.append(distance)\n",
    "            sentences[i][\"distance_to_next\"] = distance\n",
    "        return distances, sentences\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess text for sentence splitting, handling abbreviations and no space after periods.\"\"\"\n",
    "        for abbrev in self.abbreviations:\n",
    "            text = text.replace(abbrev, abbrev.replace('.', '<prd>'))\n",
    "        text = re.sub(r'(?<=[.?!])(?=[^\\s])', r' <nospace>', text)\n",
    "        return text\n",
    "\n",
    "    def post_process_chunks(self, chunks: List[str]) -> List[str]:\n",
    "        refined_chunks = []\n",
    "        for chunk in chunks:\n",
    "            if len(chunk.split()) < 3:  # Adjust the number based on your needs\n",
    "                continue\n",
    "            if chunk.startswith('<nospace>'):  # Specific pattern you might want to check\n",
    "                continue\n",
    "            # Add more conditions as needed\n",
    "            refined_chunks.append(chunk)\n",
    "        return refined_chunks\n",
    "\n",
    "\n",
    "    def split_content(self, text: str) -> List[str]:\n",
    "        if not text.strip():  # Check if the text is empty or whitespace\n",
    "            return []\n",
    "        \n",
    "        preprocessed_text = self.preprocess_text(text)\n",
    "\n",
    "        sentences = [{\"sentence\": sentence.replace('<prd>', '.').replace(' <nospace>', ''), \"index\": i}\n",
    "                    for i, sentence in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        sentences = self.combine_sentences(sentences)\n",
    "        embeddings = [self.get_embedding(sentence[\"combined_sentence\"]) for sentence in sentences]\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence[\"combined_sentence_embedding\"] = embeddings[i]\n",
    "\n",
    "        distances, _ = self.calculate_cosine_distances(sentences)\n",
    "        chunks, start_index = [], 0\n",
    "        breakpoint_percentile_threshold = self.breakpoint_percentile_threshold\n",
    "        breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "        indices_above_thresh = [i for i, distance in enumerate(distances) if distance > breakpoint_distance_threshold]\n",
    "\n",
    "        for index in indices_above_thresh:\n",
    "            group = sentences[start_index:index + 1]\n",
    "            chunks.append(\" \".join(sentence[\"sentence\"] for sentence in group))\n",
    "            start_index = index + 1\n",
    "\n",
    "        if start_index < len(sentences):\n",
    "            chunks.append(\" \".join(sentence[\"sentence\"] for sentence in sentences[start_index:]))\n",
    "\n",
    "        refined_chunks = self.post_process_chunks(chunks)\n",
    "        return refined_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intializing OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key= \"*******\")\n",
    "Chunker = SemanticChunker(embedding_client=client, breakpoint_percentile_threshold=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing the embedded chunks using Upstash Vector DB\n",
    "##### Uncomment to chunk and Index the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upstash_vector import Index\n",
    "# chunks = Chunker.split_content(text)\n",
    "index = Index(url=\"********\", token=\"**************\")\n",
    "# for chunk in chunks:\n",
    "#     index.upsert(vectors=[(str(uuid.uuid4()), Chunker.get_embedding(chunk), {\"judgment\": \"XvsY\",\"text\":chunk}),]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intialize Cohere Client \n",
    "##### Query Vectors from Upstash and Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client(\"*******\")\n",
    "\n",
    "def provide_chunks(query: str) -> List[str]:\n",
    "    results = index.query(\n",
    "        vector=Chunker.get_embedding(query),\n",
    "        top_k=25,\n",
    "        include_vectors=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    result_chunk = []\n",
    "    for result in results:\n",
    "        result_chunk.append(result.metadata['text'])\n",
    "   \n",
    "\n",
    "    response = co.rerank(\n",
    "        model='rerank-english-v2.0',\n",
    "        query=query,\n",
    "        documents=result_chunk,\n",
    "        top_n=15,\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up OpenAI API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"*********\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "As an expert law professional, you are required to deliver an accessible and to the detailed Response to the research question posed by your client. \n",
    "If you don't know the answer, you can say \"I don't know\" or \"I don't have enough information to answer this question. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Provide_Answer(question, chunks):\n",
    "   USER_PROMPT = f\"\"\"\n",
    "   Research Question: {question}\n",
    "   The List of Paragraphs are as follows:\n",
    "   {chunks}\n",
    "   Think step by step and provide the answer to the research question based on the provided information from Paragraph\n",
    "   \"\"\"\n",
    "\n",
    "   messages = [\n",
    "      {\"content\":SYSTEM_MESSAGE,\"role\":\"system\"},\n",
    "      { \"content\": USER_PROMPT,\"role\": \"user\"}\n",
    "   ]\n",
    "   response = completion(model=\"gpt-3.5-turbo-0125\", messages=messages)\n",
    "   return response.choices[0].message.content\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Who are the main parties involved in this case and the nature of their dispute?\",\n",
    "    \"What specific agreement was at the center of the Arcelor Mittal Nippon Steel India Ltd. vs. Essar Bulk Terminal Ltd. case?\",\n",
    "    \"How did the dispute in the Arcelor Mittal Nippon Steel India Ltd. vs. Essar Bulk Terminal Ltd. case reach the court?\",\n",
    "    \"What was the primary legal issue in the Arcelor Mittal Nippon Steel India Ltd. vs. Essar Bulk Terminal Ltd. case?\",\n",
    "    \"Did the case address the arbitrability of disputes in the context of a commercial agreement?\",\n",
    "    \"What role did the efficacy of Section 17 remedies play in the legal proceedings?\",\n",
    "    \"What was the petitioner's main argument regarding court intervention under Section 9 of the Arbitration Act?\",\n",
    "    \"How did the petitioner view the role of the Arbitral Tribunal in granting interim relief?\",\n",
    "    \"What legal authorities or precedents did the petitioner cite to support their arguments?\",\n",
    "    \"What stance did the respondent take regarding the arbitrability of the dispute?\",\n",
    "    \"On what grounds did the respondent seek interim measures from the court?\",\n",
    "    \"How did the respondent justify the court's power to entertain Section 9 applications after the Arbitral Tribunal's constitution?\",\n",
    "    \"Which legal precedents were analyzed regarding court's power under Section 9 after Arbitral Tribunal constitution?\",\n",
    "    \"How did precedents influence the court's view on the interaction between Sections 9 and 17 of the Arbitration Act?\",\n",
    "    \"What precedent did the court consider significant for the principle of minimal court intervention in arbitration?\",\n",
    "    \"What legal framework governs the arbitration process as discussed in the case?\",\n",
    "    \"How does the court interpret the role of Section 9 in the arbitration process?\",\n",
    "    \"What is the significance of the court's analysis on the arbitrability of disputes in commercial agreements?\",\n",
    "    \"How did the court justify its authority to grant interim measures under Section 9 after the constitution of an arbitral tribunal?\",\n",
    "    \"In what way did the court address the concerns regarding the overlap between Sections 9 and 17 of the Arbitration Act?\",\n",
    "    \"What impact does the court's decision have on the principle of minimal court intervention in arbitration?\",\n",
    "    \"Describe the nature and scope of the Cargo Handling Agreement that led to the dispute between Arcelor Mittal Nippon Steel India Ltd. and Essar Bulk Terminal Ltd.\",\n",
    "    \"What were the specific amendments to the Cargo Handling Agreement disputed by the parties?\",\n",
    "    \"How did the contractual obligations under the Cargo Handling Agreement become a point of contention?\",\n",
    "    \"What complex legal principles were at stake in this arbitration case?\",\n",
    "    \"How did the court interpret the application of Section 9 vs. Section 17 of the Arbitration and Conciliation Act in this context?\",\n",
    "    \"What was the significance of the court's interpretation of the Arbitration Act in the context of commercial arbitration?\",\n",
    "    \"How did the court justify its decision regarding the entertainability of Section 9 applications post-Arbitral Tribunal constitution?\",\n",
    "    \"What legal precedents did the court rely on to support its reasoning?\",\n",
    "    \"How did the court's reasoning address the balance between arbitration autonomy and the need for judicial intervention?\",\n",
    "    \"In what way did the court apply the principles of contract law to the dispute?\",\n",
    "    \"How did the court view the role of arbitration in resolving commercial disputes in this case?\",\n",
    "    \"What implications did the court's decisions have for the future of arbitration in commercial agreements?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Test Question set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "Answers = []\n",
    "for question in tqdm(questions, desc=\"Processing Questions\"):\n",
    "    result_chunk = provide_chunks(question)\n",
    "    Answers.append(Provide_Answer(question, result_chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intializing Eval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_similarity(answer, ref_answer, model):\n",
    "    \"\"\"\n",
    "    Calculate the semantic similarity score between an answer and a reference answer using a given model.\n",
    "\n",
    "    Args:\n",
    "    - answer (str): The first answer.\n",
    "    - ref_answer (str): The reference answer.\n",
    "    - model: The sentence-transformers model for encoding text.\n",
    "\n",
    "    Returns:\n",
    "    - float: The cosine similarity score between the answer and the reference answer.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Actual Answer: \", ref_answer)\n",
    "\n",
    "    print(\"Predicted Answer: \", answer)\n",
    "\n",
    "    # Encode the answers into embeddings\n",
    "    embeddings1 = model.encode(answer, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(ref_answer, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # Return the cosine similarity score\n",
    "    return cosine_scores.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Human Evaluated DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Human_eval.json\", 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ref_answers = data[\"qaArray\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ref_answers = [ans[\"answer\"] for ans in Ref_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing Evaluation of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(len(questions)):\n",
    "    print(f\"Question:{i}:{questions[i]}\")\n",
    "    score = calculate_semantic_similarity(Answers[i], Ref_answers[i], model)\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "    print(\"\\n========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Average Eval Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(scores)\n",
    "average = total / len(scores)\n",
    "print(\"Average Score: \", average)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
